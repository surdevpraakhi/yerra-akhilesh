\let\negmedspace\undefined
\let\negthickspace\undefined
\documentclass[journal]{IEEEtran}
\usepackage[a5paper, margin=10mm, onecolumn]{geometry}
\usepackage{tfrupee} % Include tfrupee package

\setlength{\headheight}{1cm} % Set the height of the header box
\setlength{\headsep}{0mm}     % Set the distance between the header box and the top of the text

\usepackage{gvv-book}
\usepackage{gvv}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{txfonts}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{gensymb}
\usepackage{comment}
\usepackage[breaklinks=true]{hyperref}
\usepackage{tkz-euclide} 
\usepackage{listings}
\def\inputGnumericTable{}
\usepackage[latin1]{inputenc}
\usepackage{color}
\usepackage{array}
\usepackage{longtable}
\usepackage{calc}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{ifthen}
\usepackage{lscape}

% Fixing the alignment of section title
\usepackage{titlesec}
\titleformat{\section}[block]{\normalfont\bfseries}{\thesection}{1em}{}
\titlespacing*{\section}{0pt}{*1}{*1} % Adjusts spacing before and after the section title

\begin{document}

\bibliographystyle{IEEEtran}
\vspace{3cm}

\title{Software Assignment}
\author{EE24BTECH11066 - YERRA AKHILESH
}
% Suppress page break after title
{\let\newpage\relax\maketitle}

\renewcommand{\thefigure}{\theenumi}
\renewcommand{\thetable}{\theenumi}
\setlength{\intextsep}{10pt} % Space between text and floats

% Number equations and figures within enumerated lists
\numberwithin{equation}{enumi}
\numberwithin{figure}{enumi}
\renewcommand{\thetable}{\theenumi}

% Section and Subsection
\section{Eigenvalue} % Section title fixed to align as expected
\subsection{Expression}

\begin{itemize}
    \item Consider an $n\times n$  matrix $\mathbf{A}$ and a nonzero vector $\mathbf{v}$ of length $n$.  If multiplying $\mathbf{A}$ with $\mathbf{v}  (\text{denoted by }  \mathbf{A} \mathbf{v} )$ simply scales $\mathbf{v}$  by a factor of $\lambda$, where $\lambda$ is a scalar, then $\mathbf{v}$ is called an eigenvector of $\mathbf{A}$, and $\lambda$ is the corresponding eigenvalue. This relationship can be expressed as: 
\end{itemize}

\begin{align*}
    \mathbf{A} \mathbf{v} = \mathbf{\lambda} \mathbf{v}
\end{align*}


\begin{itemize}
    \item expressing the above equation into matrix form:
\end{itemize}
\begin{align*}
\begin{bmatrix}
A_{11} & A_{12} & \cdots & A_{1n} \\
A_{21} & A_{22} & \cdots & A_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
A_{n1} & A_{n2} & \cdots & A_{nn}
\end{bmatrix}
\begin{bmatrix}
v_1 \\ 
v_2 \\ 
\vdots \\ 
v_n
\end{bmatrix}
=\lambda
\begin{bmatrix}
v_1 \\ 
v_2 \\ 
\vdots \\ 
v_n
\end{bmatrix}
\end{align*}
\begin{itemize}
    \item represented as:
\end{itemize}
\begin{align*}
    \brak{\mathbf{A} - \lambda \mathbf{I}} \mathbf{v} = \mathbf{0},
\end{align*}
\begin{itemize}
    \item above equation has a nonzero solution $v$ if and only if the determinant of the matrix $\brak{A-\lambda I}$ is zero. Therefore, the eigenvalues of $A$ are values of $\lambda$ that satisfy the equation
\end{itemize}

\begin{align*}
    \text{det}(\mathbf{A}-\lambda \mathbf{I})=0
\end{align*}
\section{Algorithms}
These are some algorithms for finding Eigenvalues of a given matrix:
\begin{itemize}
    \item QR decomposition
    \item Power Iteration
    \item Lanczos Algorithm
    \item Jacobi Algorithm
\end{itemize}
\section{Chosen Algorithm}
\subsection{QR decomposition\\}
\subsubsection{Introduction\\}
\begin{itemize}
    \item Main idea of QR decomposition is to repeatedly decompose a matrix $\mathbf{A}$ into product of an orthogonal matrix $\mathbf{Q} \brak{\text{i.e}.,\mathbf{Q^{T}Q=I}}$ and upper triangular matrix $\mathbf{R}$, \\
    
    Decompose $\mathbf{A}$ into its QR decomposition:
\end{itemize}
\begin{align*}
    \mathbf{A=QR}
\end{align*}
\begin{itemize}
    \item By applying QR algorithm iteratively, the matrix $\mathbf{A}$ is converted into a diagonal matrix, in which diagonal elements are the required eigenvalues of the given matrix $\mathbf{A}$.\\ 
\end{itemize}

\subsubsection{performing QR decomposition\\}
\begin{itemize}
    \item $\mathbf{A_0}$ is a $n \times n$ matrix, by performing QR decomposition $\mathbf{A_0}$ expressed as:
    \begin{align*}
        \mathbf{A_0 = Q_{0}R_{0}}
    \end{align*}
    \item form a new matrix $\mathbf{A_1}$ as:
    
\end{itemize}
\begin{align*}
    \mathbf{A_1 = R_{0}Q_{0}}
\end{align*}
\begin{itemize}
    \item $\mathbf{A_1}$ is such a matrix that having same eigenvalues as that of $\mathbf{A_{0}}$.
    \\
\end{itemize}
\subsubsection{Repeat QR decomposition}

\begin{align*}
    \mathbf{A_1=Q_1 R_1}
\end{align*}
\begin{itemize}
    \item $\mathbf{A_2}$ can be expressed interms of $R_{1}, Q_{1}$ as

\begin{align*}
    \mathbf{A_2 = R_{1} Q_{1}}
\end{align*}
   \item continue this iterative decomposition until $\mathbf{A_k}$ turns into upper triangular matrix.\\
\end{itemize}


\subsubsection{Extracting eigenvalues}
\begin{itemize}
    \item Diagonal entrities of $\mathbf{A_k}$ matrix are the required eigenvalues of a matrix $\mathbf{A_0}$ - as the diagonal entrities of a uppertriangular matrix are the eigenvalues of that matrix\\
    $\mathbf{A_k}$ is a matrix of same eigenvalues as $\mathbf{A_0}$ as we discussed earlier.\\
\end{itemize}
\subsection{QR decomposition using Householder Reflections}
\textbf{Algorithm:}
Given a square matrix \( A \in \mathbb{R}^{n \times n} \), the QR decomposition with Householder reflections computes the orthogonal matrix \( Q \) and upper triangular matrix \( R \) such that \( A = QR \).

\textbf{Steps:}
\begin{enumerate}
    \item Initialize \( Q = I_n \) (identity matrix of size \( n \)) and \( R = A \).
    \item For \( k = 1, \ldots, n-1 \):
    \begin{enumerate}
        \item Extract \( x = R[k:n, k] \).
        \item Compute the Householder vector \( v \):
        \[
        v = x + \text{sign}(x_1) \|x\|_2 e_1, \quad v = \frac{v}{\|v\|_2}.
        \]
        \item Form the Householder matrix:
        \[
        H = I - 2vv^T.
        \]
        \item Update \( R \): \( R = HR \).
        \item Update \( Q \): \( Q = QH^T \).
    \end{enumerate}
    \item After \( n-1 \) iterations, \( Q \) and \( R \) satisfy \( A = QR \).
\end{enumerate}

\textbf{Eigenvalue Iteration:}
\begin{enumerate}
    \item Initialize \( A_0 = A \).
    \item For \( i = 1, 2, \ldots \) (until convergence):
    \begin{enumerate}
        \item Compute the QR decomposition \( Q_iR_i \) of \( A_{i-1} \).
        \item Update \( A_i = R_iQ_i \).
    \end{enumerate}
    \item The diagonal elements of \( A_i \) converge to the eigenvalues of \( A \).
\end{enumerate}
\section{Time complexity}

\begin{itemize}
    \item Time complexity of QR algorithm is $O\brak{n^3}$\\
\end{itemize}

\section{Other insights}
\subsection{About QR algorithm}
\begin{itemize}
    \item The QR decomposition is a powerful method for computing eigenvalues of a matrix. When used iteratively, the QR algorithm (which is based on QR decomposition) can be helpful to find all eigenvalues of a matrix, even for large matrices, in a stable and efficient manner.
\end{itemize}
\subsection{specific types of matrices}
\subsubsection{square matrix}
\begin{itemize}
    \item QR decomposition is suitable not only for symmetric matrices but also for non-symmetric matrices.\\
\end{itemize}
\subsubsection{rectangular matrix}
\begin{itemize}
    \item The QR algorithm is specifically meant for finding eigenvalues of square matrices, so it doesn't directly apply to rectangular ones. However, ideas from QR decomposition are still valuable for working with rectangular matrices, such as in solving least-squares problems or computing singular values. For a more direct approach to analyzing rectangular matrices, methods like singular value decomposition (SVD) or other matrix factorizations are generally used.\\
\end{itemize}
\subsubsection{special matrix - sparse matrix}
\begin{itemize}
    \item QR decomposition is particularly good at keeping the matrix sparse during computations, especially when using methods like Givens rotations or Householder transformations. This is a big advantage for large, sparse matrices because it helps save both memory and processing power.
\end{itemize}
\subsection{convergence rate}
\begin{itemize}
    \item The QR algorithm has a rapid rate of convergence, specifically quadratic convergence. This means that with each iteration, the accuracy of the eigenvalues improves significantly, often doubling the number of correct digits. For matrices that are well-conditioned, the algorithm usually requires only a relatively small number of iterations to produce accurate results, typically between 10 and 20 even for large matrices.
\end{itemize}
\subsection{Memory usage}
\begin{itemize}
    \item The memory usage of the QR algorithm for finding eigenvalues of $n \times n$ matrix is $O\brak{n^2}$
\end{itemize}
\section{Comparing the Algorithms}

\subsection{QR decomposition\\}
\subsubsection{Pros}
\begin{itemize}
    \item Works for all square matrices $\brak{\text{real, complex, symmetric, non-symmetric}}$
    \item Computes all eigenvalues simultaneously.
    \item Maintains accuracy even for ill-conditioned matrices.
\end{itemize}
\subsubsection{Cons}
\begin{itemize}
    \item Requires multiple iterations to converge.
    \item The algorithm requires the storage of intermediate matrices and calculations, which can be memory-intensive for large matrices.
    \item Time complexity 
\end{itemize}
\subsection{Power Iteration}
\begin{itemize}
    \item A method to find the largest eigenvalue and corresponding eigenvector of a matrix by repeatedly multiplying it by a vector and normalizing the result.\\
\end{itemize}
\subsubsection{Pros}
\begin{itemize}
    \item Suitable for finding the largest eigenvalue of a sparse matrix.
    \item Memory-efficient: Only requires storage for the matrix and a single vector.
\end{itemize}
\subsubsection{Cons}
\begin{itemize}
    \item Only finds the eigenvalue with largest magnitude.
    \item May require many iterations for matrices with eigenvalues close in magnitude.
    
\end{itemize}
\subsection{Lanczos Algorithm}
\begin{itemize}
    \item A technique to approximate a few eigenvalues and eigenvectors of a large symmetric matrix efficiently by reducing it to a smaller tridiagonal form.\\
\end{itemize}
\subsubsection{Pros}
\begin{itemize}
    \item Finds smallest and largest eigenvalues effectively.
    \item Reduces computation by working on a smaller tridiagonal matrix.
\end{itemize}
\subsubsection{Cons}
\begin{itemize}
    \item It fails for non-symmetric matrices.
    \item loss of orthogonality among the Lanczos vectors due to finite-precision arithmetic
\end{itemize}
\subsection{Jacobi Method}
\begin{itemize}
    \item  A method for finding all eigenvalues and eigenvectors of a symmetric matrix by repeatedly rotating the matrix to zero out off-diagonal elements.\\
\end{itemize}
\subsubsection{Pros}
\begin{itemize}
    \item It works very effectively for symmetric matrices.
\end{itemize}
\subsubsection{Cons}
\begin{itemize}
    \item Only suitable for symmetric matrices.
\end{itemize}
\begin{table}[h!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Method}            & \textbf{Type of Matrix}  & \textbf{Main Use}                            & \textbf{Complexity}           \\ \hline
Power Iteration            & Any matrix              & Dominant eigenvalue                          & $O(n^2)$ per iteration        \\ \hline
QR Algorithm               & Any matrix              & All eigenvalues                              & $O(n^3)$                      \\ \hline
Lanczos Algorithm          & Sparse, symmetric       & Large sparse matrices                        & $O(n^2)$                      \\ \hline
Jacobi Algorithm           & Symmetric               & All eigenvalues                              & $O(n^3)$                      \\ \hline
\end{tabular}%
}
\label{tab:eigenvalue_methods}
\end{table}
\section{CONCLUSION - Why QR!}
In the realm of eigenvalue computation, QR decomposition stands as the most reliable and versatile method. It strikes a perfect balance between accuracy, stability, and adaptability, making it suitable for a wide range of applications. Its iterative refinement ensures convergence to precise eigenvalues, delivering results that are both consistent and mathematically rigorous.

While algorithms like Lanczos or the Power method may perform well in specific cases, they often lack the robustness required for broader applications. QR decomposition, on the other hand, excels due to its ability to handle dense, non-symmetric, and complex matrices effectively, making it a cornerstone of numerical computation.

Opting for QR decomposition is more than just a practical decision-it represents a commitment to achieving trustworthy and high-quality results. It sets the benchmark for eigenvalue analysis, proving itself to be an indispensable tool in computational mathematics.\\

\begin{thebibliography}{9}

\bibitem{strang}
G. Strang, 
\textit{Linear Algebra and Its Applications}, 
4th ed., Cengage Learning, 2006. \\
A comprehensive resource for understanding linear algebra concepts, including eigenvalues and eigenvectors.

\bibitem{trefethen}
L. N. Trefethen and D. Bau III, 
\textit{Numerical Linear Algebra}, 
SIAM, 1997. \\
Explains numerical methods, including QR decomposition, for computing eigenvalues.

\bibitem{golub}
G. H. Golub and C. F. Van Loan, 
\textit{Matrix Computations}, 
4th ed., Johns Hopkins University Press, 2013. \\
Covers advanced algorithms such as QR decomposition, Power Iteration, and the Lanczos Algorithm.

\bibitem{eig_tutorial}
Eigenvalue and Eigenvector Tutorial, \\
\url{https://www.mathsisfun.com/algebra/eigenvalue.html}. \\
A beginner-friendly online resource for eigenvalues and eigenvectors.

\bibitem{qr_algorithms}
QR Algorithm Overview, \\
\url{https://en.wikipedia.org/wiki/QR_algorithm}. \\
Details the QR decomposition process and its applications in eigenvalue computation.

\end{thebibliography}


\end{document}

